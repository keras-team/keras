============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.1, pluggy-1.6.0 -- /mnt/data/keras/venv/bin/python3
cachedir: .pytest_cache
rootdir: /mnt/data/keras
configfile: pyproject.toml
plugins: cov-7.0.0
collecting ... collected 1 item

keras/src/utils/jax_layer_test.py::TestJaxLayer::test_flax_layer_training_independent_bound_method Model: "FlaxTrainingIndependentModel1"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_layer (InputLayer)        â”‚ (None, 28, 28, 1)      â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flax_layer (FlaxLayer)          â”‚ (None, 10)             â”‚       648,226 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 648,226 (2.47 MB)
 Trainable params: 648,226 (2.47 MB)
 Non-trainable params: 0 (0.00 B)
[1m 1/10[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m23s[0m 3s/step - categorical_accuracy: 0.0312 - loss: 2.3293[1m10/10[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 7ms/step - categorical_accuracy: 0.0813 - loss: 2.3713
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 1s/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 1s/step
Model: "FlaxTrainingIndependentModel2"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ flax_layer_1 (FlaxLayer)        â”‚ (None, 10)             â”‚       648,226 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 648,226 (2.47 MB)
 Trainable params: 648,226 (2.47 MB)
 Non-trainable params: 0 (0.00 B)
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 1s/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 1s/step
Model: "FlaxTrainingIndependentModel2"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ flax_layer_1 (FlaxLayer)        â”‚ (None, 10)             â”‚       648,226 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 1,944,680 (7.42 MB)
 Trainable params: 648,226 (2.47 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,296,454 (4.95 MB)
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 1s/step[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 1s/step
Saved artifact at '/mnt/data/tmp/tmpvyxw070h/jax_layer_export'. The following endpoints are available:

* Endpoint 'serve'
  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor_8')
Output Type:
  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)
Captures:
  126200239376720: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239374800: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239377872: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239374416: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239375184: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239377104: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239378064: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239378256: TensorSpec(shape=(), dtype=tf.resource, name=None)
  126200239378640: TensorSpec(shape=(), dtype=tf.resource, name=None)
FAILED

=================================== FAILURES ===================================
________ TestJaxLayer.test_flax_layer_training_independent_bound_method ________

self = <keras.src.utils.jax_layer_test.TestJaxLayer testMethod=test_flax_layer_training_independent_bound_method>
flax_model_class = <class 'keras.src.utils.jax_layer_test.FlaxTrainingIndependentModel'>
flax_model_method = 'forward', init_kwargs = {}, trainable_weights = 8
trainable_params = 648226, non_trainable_weights = 0, non_trainable_params = 0

    @parameterized.named_parameters(
        {
            "testcase_name": "training_independent_bound_method",
            "flax_model_class": "FlaxTrainingIndependentModel",
            "flax_model_method": "forward",
            "init_kwargs": {},
            "trainable_weights": 8,
            "trainable_params": 648226,
            "non_trainable_weights": 0,
            "non_trainable_params": 0,
        },
        {
            "testcase_name": "training_rng_unbound_method",
            "flax_model_class": "FlaxDropoutModel",
            "flax_model_method": None,
            "init_kwargs": {
                "method": "flax_dropout_wrapper",
            },
            "trainable_weights": 8,
            "trainable_params": 648226,
            "non_trainable_weights": 0,
            "non_trainable_params": 0,
        },
        {
            "testcase_name": "training_rng_state_no_method",
            "flax_model_class": "FlaxBatchNormModel",
            "flax_model_method": None,
            "init_kwargs": {},
            "trainable_weights": 13,
            "trainable_params": 354258,
            "non_trainable_weights": 8,
            "non_trainable_params": 536,
        },
        {
            "testcase_name": "training_rng_unbound_method_dtype_policy",
            "flax_model_class": "FlaxDropoutModel",
            "flax_model_method": None,
            "init_kwargs": {
                "method": "flax_dropout_wrapper",
                "dtype": DTypePolicy("mixed_float16"),
            },
            "trainable_weights": 8,
            "trainable_params": 648226,
            "non_trainable_weights": 0,
            "non_trainable_params": 0,
        },
    )
    @pytest.mark.skipif(flax is None, reason="Flax library is not available.")
    def test_flax_layer(
        self,
        flax_model_class,
        flax_model_method,
        init_kwargs,
        trainable_weights,
        trainable_params,
        non_trainable_weights,
        non_trainable_params,
    ):
        flax_model_class = FLAX_OBJECTS.get(flax_model_class)
        if "method" in init_kwargs:
            init_kwargs["method"] = FLAX_OBJECTS.get(init_kwargs["method"])
    
        def create_wrapper(**kwargs):
            params = kwargs.pop("params") if "params" in kwargs else None
            state = kwargs.pop("state") if "state" in kwargs else None
            if params and state:
                variables = {**params, **state}
            elif params:
                variables = params
            elif state:
                variables = state
            else:
                variables = None
            kwargs["variables"] = variables
            flax_model = flax_model_class()
            if flax_model_method:
                kwargs["method"] = getattr(flax_model, flax_model_method)
            return FlaxLayer(flax_model_class(), **kwargs)
    
>       self._test_layer(
            flax_model_class.__name__,
            create_wrapper,
            init_kwargs,
            trainable_weights,
            trainable_params,
            non_trainable_weights,
            non_trainable_params,
        )

keras/src/utils/jax_layer_test.py:505: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/src/utils/jax_layer_test.py:348: in _test_layer
    output4 = model4.serve(x_test)
              ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_restored_function_body_799', num_outputs = 1
inputs = [<tf.Tensor: shape=(32, 28, 28, 1), dtype=float32, numpy=
array([[[[0.36329234],
         [0.8334724 ],
         [0.73...ce:CPU:0", container="Anonymous", type="tensorflow::Var", dtype and shapes : "[ DType enum: 1, Shape: [200] ]")>>, ...]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x00\n\x0e\n\nTPU_SYSTEM\x10\x012\x02J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x72c7463c8450>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.NotFoundError: Graph execution error:
E                                           
E                                           Detected at node XlaCallModule defined at (most recent call last):
E                                           <stack traces unavailable>
E                                           could not find registered transfer manager for platform Host -- check target linkage
E                                           	 [[{{node XlaCallModule}}]] [Op:__inference_restored_function_body_799]

venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: NotFoundError
------------------------------ Captured log call -------------------------------
WARNING  absl:function_deserialization.py:672 Importing a function (__inference_internal_grad_fn_298) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
=========================== short test summary info ============================
FAILED keras/src/utils/jax_layer_test.py::TestJaxLayer::test_flax_layer_training_independent_bound_method - tensorflow.python.framework.errors_impl.NotFoundError: Graph execution error:

Detected at node XlaCallModule defined at (most recent call last):
<stack traces unavailable>
could not find registered transfer manager for platform Host -- check target linkage
	 [[{{node XlaCallModule}}]] [Op:__inference_restored_function_body_799]
============================== 1 failed in 28.43s ==============================
